{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88923224",
   "metadata": {},
   "source": [
    "# HR Analytics: Understanding Employee Attrition\n",
    "Analyzing trends and patterns in employee attrition using HR analytics data from Kaggle. The data is on Atlas Lab employees.\n",
    "\n",
    "## Objectives\n",
    "- Understand the distribution of employee attributes (age, department, role, etc.)\n",
    "- Explore patterns in employee attrition\n",
    "- Analyze job and performance satisfaction metrics\n",
    "- Identify factors correlated with attrition\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88276d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis, normaltest, probplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db057f4",
   "metadata": {},
   "source": [
    "## Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_employee = pd.read_csv('Employee.csv')\n",
    "df_performance = pd.read_csv('PerformanceRating.csv')\n",
    "\n",
    "# Merge Data\n",
    "df_combined = pd.merge(df_employee, df_performance, on='EmployeeID')\n",
    "print(df_combined.shape)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405a673",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- dropping unnecessary columns \n",
    "- fixing column types\n",
    "- mapping binary and ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883349c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "print(df_combined.isnull().sum())\n",
    "\n",
    "# Map binary values\n",
    "df_combined['Attrition'] = df_combined['Attrition'].map({'Yes':1, 'No': 0})\n",
    "df_combined['OverTime'] = df_combined['OverTime'].map({'Yes': 1, 'No':0})\n",
    "\n",
    "\n",
    "# Convert ordinal values\n",
    "from pandas.api.types import CategoricalDtype\n",
    "levels = CategoricalDtype(\n",
    "    categories=[1,2,3,4,5],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "cols_to_convert = [\n",
    "    'EnvironmentSatisfaction',\n",
    "    'JobSatisfaction',\n",
    "    'RelationshipSatisfaction',\n",
    "    'WorkLifeBalance',\n",
    "    'SelfRating',\n",
    "    'ManagerRating',\n",
    "    'Education',\n",
    "]\n",
    "for col in cols_to_convert:\n",
    "    df_combined[col] = df_combined[col].astype(levels)\n",
    "\n",
    "# convert remaining categorical columns\n",
    "categorical_cols = ['BusinessTravel', 'Department', 'State', 'Ethnicity', 'EducationField', 'JobRole', \n",
    "                    'MaritalStatus', 'StockOptionLevel', 'TrainingOpportunitiesWithinYear',\n",
    "                    'TrainingOpportunitiesTaken'\n",
    "                    ]\n",
    "for col in categorical_cols:\n",
    "    df_combined[col] = df_combined[col].astype('category')\n",
    "\n",
    "# Date columns\n",
    "df_combined['HireDate'] = pd.to_datetime(df_combined['HireDate'])\n",
    "df_combined['ReviewDate'] = pd.to_datetime(df_combined['ReviewDate'])\n",
    "\n",
    "# Drop unused columns\n",
    "drop_columns = ['FirstName', 'LastName', 'PerformanceID']\n",
    "\n",
    "df_combined = df_combined.drop(drop_columns, axis=1)\n",
    "\n",
    "# Save cleaned data\n",
    "df_combined.to_csv('cleaned_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78904f7f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis - Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visual style\n",
    "sns.set(style='whitegrid', palette='dark')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load the dataset\n",
    "df = df_combined.copy()\n",
    "\n",
    "print(df.shape)\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea0fbb",
   "metadata": {},
   "source": [
    "## Function 1: ql_stats\n",
    "- a function to handle qualitative columns\n",
    "- runs summary (counts & percentages) for the column parameter\n",
    "- displays how many unique responses a category has, the most occurring response, and how many employees chose that response.\n",
    "- plots countplot for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ql_stats(df, col):\n",
    "    \"\"\"\n",
    "    Prints the summary and percentage of each category \n",
    "    in the specified column w/count plots.\n",
    "\n",
    "    Parameters: \n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    col (str): The name of the column to summarize.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Categorical Summary: {col} ---\")\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    percentages = df[col].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentages': percentages.round(2)\n",
    "    })\n",
    "    print(summary)\n",
    "    print(f\"Unique categories: {df[col].nunique(dropna=False)}\")\n",
    "    print(f\"Most frequent: {df[col].mode()[0]}\")\n",
    "\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966af95",
   "metadata": {},
   "source": [
    "## Function 2: qn_stats\n",
    "- a function to handle quantitative data \n",
    "- displays a general summary for the column parameter\n",
    "- skewness: tells us the shape and spread of the data from the center\n",
    "- kurtosis: tells us how much data is far from the center\n",
    "- plots histogram + kde\n",
    "- runs normality test to see if the data is normally distributive\n",
    "- plots QQ-plot to visualize how much the data follows the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b74ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qn_stats(df, col):\n",
    "    \"\"\"\n",
    "    Prints out summary for quantitative columns and creates \n",
    "    histogram + KDE plots.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    col (str): The name of the numeric column to summarize.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Numerical Summary: {col} ---\")\n",
    "    desc = df[col].describe()\n",
    "    print(desc)\n",
    "\n",
    "    print(f\"Mode: {df[col].mode()[0]}\")\n",
    "    print(f\"Skewness: {skew(df[col].dropna()):.2f}\")\n",
    "    print(f\"Kurtosis: {kurtosis(df[col].dropna()):.2f}\")\n",
    "    \n",
    "    # Histogram + KDE\n",
    "    sns.histplot(df[col], kde=True, bins=20)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Normality Test\n",
    "    stat, p=normaltest(df[col])\n",
    "    print(f\"\\nD'Agostino and Pearson Test:\")\n",
    "    print(f\" Statistic = {stat:.4f}, p-value = {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(\"Data is not normally distributed.\")\n",
    "    else:\n",
    "        print(\"Data is normally distributed.\")\n",
    "\n",
    "    # QQ Plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    probplot(df[col], dist='norm', plot=plt)\n",
    "    plt.title(f'QQ-Plot of {col}')\n",
    "    plt.xlabel(\"Theoretical Quantiles\")\n",
    "    plt.ylabel('Sample Quantiles')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041df5b7",
   "metadata": {},
   "source": [
    "## Function 3: dt_stats\n",
    "- properly handles datetime data type variables\n",
    "- summarizes the values under each datetime variable.\n",
    "- visualizes monthly and yearly time series plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19899e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_stats(df, col):\n",
    "    \"\"\"\n",
    "    Summarizes datetime columns and plots time series trends.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    col (str): The name of the numeric column to summarize.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Datetime Summary: {col} ---\")\n",
    "    print(f\"Min date: {df[col].min()}\")\n",
    "    print(f\"Max date: {df[col].max()}\")\n",
    "    print(f\"Unique dates: {df[col].nunique(dropna=False)}\")\n",
    "\n",
    "    # Monthly counts\n",
    "    dt_monthly = df.set_index(col).resample('M').size()\n",
    "    plt.figure(figsize=(14,7))\n",
    "    dt_monthly.plot(marker='o')\n",
    "    plt.title(f\"Monthly count of {col}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Yearly counts\n",
    "    dt_yearly = df.set_index(col).resample('Y').size()\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    dt_yearly.plot(marker='o')\n",
    "    plt.title(f\"Yearly Count of {col}\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13d5bd",
   "metadata": {},
   "source": [
    "Using a for loop to go through all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc60ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Employee ID from the columns\n",
    "df = df.drop(columns=['EmployeeID'])\n",
    "\n",
    "# Temporarily turns Attrition and OverTime variables as categories\n",
    "df['Attrition'] = df['Attrition'].astype('category')\n",
    "df['OverTime'] = df['OverTime'].astype('category')\n",
    "\n",
    "# Loop through all columns and run stats, tests, and plots\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "        dt_stats(df, col)\n",
    "    elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "        qn_stats(df, col)\n",
    "    else:\n",
    "        ql_stats(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b4f3b",
   "metadata": {},
   "source": [
    "## Key Observations:\n",
    "- There is an even amount of female and male employees at this company (46.07% and 44.24%, respectively)\n",
    "- The distribution of age is right skewed (skewness=0.67), suggesting the company mostly hires younger people who may be at the start of their careers.\n",
    "- This company mostly requires employees to have some travel.\n",
    "- Most of the roles in this company are in the tech department.\n",
    "- The distance from work for employees is varied, there are no heavy end tails for this variable.\n",
    "- Employees are mostly from California.\n",
    "- Most people who work for this company are caucasian. \n",
    "- Most employees have a bachelor's degree.\n",
    "- The salary of employees is heavily right skewed, meaning most make 100k yearly.\n",
    "- About more than half (66.66%) employees do not work overtime.\n",
    "- This company hired most of their employees around 2012, which was when the company first started.\n",
    "- The amount of employees that were hired after 2012, harshly decreases as time goes on.\n",
    "- 66.3% of employees have not left the company since being hired. This could explain why there is a drastic change of new employees.\n",
    "- Most of the company's workforce has stayed for 10 years. Again, this makes sense since a large part of the company was hired in 2012, with the rate of new employees being hired decreasing.\n",
    "- Surprisingly, most of the employees have only spent 0-1 years in their most recent role. Since we know there's less people being hired in 2022, we can say most of the employees have recently changed their role over the past year.\n",
    "- Most employees have been promoted recently, this explains why most have changed their role in the last graph.\n",
    "- Along with more employees getting promoted recently, most employees' managers have changed over the past year.\n",
    "- The majority of employees have recently been reviewed for their performance just last month of when this data was compiled.\n",
    "- There is about an even amount of training opportunities offered yearly, however, most have taken it 0-1 times over the course of their time employed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d7469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
