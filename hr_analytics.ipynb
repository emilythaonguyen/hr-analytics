{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88923224",
   "metadata": {},
   "source": [
    "# HR Analytics: Understanding Employee Attrition\n",
    "Analyzing trends and patterns in employee attrition using HR analytics data from Kaggle. The data is on Atlas Lab employees.\n",
    "\n",
    "## Objectives\n",
    "- Understand the distribution of employee attributes (age, department, role, etc.)\n",
    "- Explore patterns in employee attrition\n",
    "- Analyze job and performance satisfaction metrics\n",
    "- Identify factors correlated with attrition\n",
    "\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88276d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis, normaltest, probplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db057f4",
   "metadata": {},
   "source": [
    "## Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8dc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "df_employee = pd.read_csv('Employee.csv')\n",
    "df_performance = pd.read_csv('PerformanceRating.csv')\n",
    "\n",
    "# Merge Data\n",
    "df_combined = pd.merge(df_employee, df_performance, on='EmployeeID')\n",
    "print(df_combined.shape)\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c405a673",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "- dropping unnecessary columns \n",
    "- fixing column types\n",
    "- mapping binary and ordinal values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883349c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "print(df_combined.isnull().sum())\n",
    "\n",
    "# Map binary values\n",
    "df_combined['Attrition'] = df_combined['Attrition'].map({'Yes':1, 'No': 0})\n",
    "df_combined['OverTime'] = df_combined['OverTime'].map({'Yes': 1, 'No':0})\n",
    "\n",
    "\n",
    "# Convert ordinal values\n",
    "from pandas.api.types import CategoricalDtype\n",
    "levels = CategoricalDtype(\n",
    "    categories=[1,2,3,4,5],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "cols_to_convert = [\n",
    "    'EnvironmentSatisfaction',\n",
    "    'JobSatisfaction',\n",
    "    'RelationshipSatisfaction',\n",
    "    'WorkLifeBalance',\n",
    "    'SelfRating',\n",
    "    'ManagerRating',\n",
    "    'Education',\n",
    "]\n",
    "for col in cols_to_convert:\n",
    "    df_combined[col] = df_combined[col].astype(levels)\n",
    "\n",
    "# convert remaining categorical columns\n",
    "categorical_cols = ['BusinessTravel', 'Department', 'State', 'Ethnicity', 'EducationField', 'JobRole', \n",
    "                    'MaritalStatus', 'StockOptionLevel', 'TrainingOpportunitiesWithinYear',\n",
    "                    'TrainingOpportunitiesTaken'\n",
    "                    ]\n",
    "for col in categorical_cols:\n",
    "    df_combined[col] = df_combined[col].astype('category')\n",
    "\n",
    "# Date columns\n",
    "df_combined['HireDate'] = pd.to_datetime(df_combined['HireDate'])\n",
    "df_combined['ReviewDate'] = pd.to_datetime(df_combined['ReviewDate'])\n",
    "\n",
    "# Drop unused columns\n",
    "drop_columns = ['FirstName', 'LastName', 'PerformanceID']\n",
    "\n",
    "df_combined = df_combined.drop(drop_columns, axis=1)\n",
    "\n",
    "# Save cleaned data\n",
    "df_combined.to_csv('cleaned_employee_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78904f7f",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Exploratory Data Analysis - Overview of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85acb2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set visual style\n",
    "sns.set(style='whitegrid', palette='dark')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Load the dataset\n",
    "df = df_combined.copy()\n",
    "\n",
    "print(df.shape)\n",
    "df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ea0fbb",
   "metadata": {},
   "source": [
    "## Function 1: ql_stats\n",
    "- a function to handle qualitative columns\n",
    "- runs summary (counts & percentages) for the column parameter\n",
    "- displays how many unique responses a category has, the most occurring response, and how many employees chose that response.\n",
    "- plots countplot for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9deb4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ql_stats(df, col):\n",
    "    \"\"\"\n",
    "    Prints the summary and percentage of each category \n",
    "    in the specified column w/count plots.\n",
    "\n",
    "    Parameters: \n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    col (str): The name of the column to summarize.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Categorical Summary: {col} ---\")\n",
    "    counts = df[col].value_counts(dropna=False)\n",
    "    percentages = df[col].value_counts(normalize=True, dropna=False) * 100\n",
    "\n",
    "    summary = pd.DataFrame({\n",
    "        'Count': counts,\n",
    "        'Percentages': percentages.round(2)\n",
    "    })\n",
    "    print(summary)\n",
    "    print(f\"Unique categories: {df[col].nunique(dropna=False)}\")\n",
    "    print(f\"Most frequent: {df[col].mode()[0]}\")\n",
    "\n",
    "    sns.countplot(x=col, data=df)\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966af95",
   "metadata": {},
   "source": [
    "## Function 2: qn_stats\n",
    "- a function to handle quantitative data \n",
    "- displays a general summary for the column parameter\n",
    "- skewness: tells us the shape and spread of the data from the center\n",
    "- kurtosis: tells us how much data is far from the center\n",
    "- plots histogram + kde\n",
    "- runs normality test to see if the data is normally distributive\n",
    "- plots QQ-plot to visualize how much the data follows the normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b74ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qn_stats(df, col):\n",
    "    \"\"\"\n",
    "    Prints out summary for quantitative columns and creates \n",
    "    histogram + KDE plots.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    col (str): The name of the numeric column to summarize.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Numerical Summary: {col} ---\")\n",
    "    desc = df[col].describe()\n",
    "    print(desc)\n",
    "\n",
    "    print(f\"Mode: {df[col].mode()[0]}\")\n",
    "    print(f\"Skewness: {skew(df[col].dropna()):.2f}\")\n",
    "    print(f\"Kurtosis: {kurtosis(df[col].dropna()):.2f}\")\n",
    "    \n",
    "    # Histogram + KDE\n",
    "    sns.histplot(df[col], kde=True, bins=20)\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "    # Normality Test\n",
    "    stat, p=normaltest(df[col])\n",
    "    print(f\"\\nD'Agostino and Pearson Test:\")\n",
    "    print(f\" Statistic = {stat:.4f}, p-value = {p:.4f}\")\n",
    "    if p < 0.05:\n",
    "        print(\"Data is not normally distributed.\")\n",
    "    else:\n",
    "        print(\"Data is normally distributed.\")\n",
    "\n",
    "    # QQ Plot\n",
    "    plt.figure(figsize=(6,6))\n",
    "    probplot(df[col], dist='norm', plot=plt)\n",
    "    plt.title(f'QQ-Plot of {col}')\n",
    "    plt.xlabel(\"Theoretical Quantiles\")\n",
    "    plt.ylabel('Sample Quantiles')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041df5b7",
   "metadata": {},
   "source": [
    "## Function 3: dt_stats\n",
    "- properly handles datetime data type variables\n",
    "- summarizes the values under each datetime variable.\n",
    "- visualizes monthly and yearly time series plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19899e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_stats(df, col):\n",
    "    \"\"\"\n",
    "    Summarizes datetime columns and plots time series trends.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The DataFrame containing the data.\n",
    "    col (str): The name of the numeric column to summarize.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Datetime Summary: {col} ---\")\n",
    "    print(f\"Min date: {df[col].min()}\")\n",
    "    print(f\"Max date: {df[col].max()}\")\n",
    "    print(f\"Unique dates: {df[col].nunique(dropna=False)}\")\n",
    "\n",
    "    # Monthly counts\n",
    "    dt_monthly = df.set_index(col).resample('M').size()\n",
    "    plt.figure(figsize=(14,7))\n",
    "    dt_monthly.plot(marker='o')\n",
    "    plt.title(f\"Monthly count of {col}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Yearly counts\n",
    "    dt_yearly = df.set_index(col).resample('Y').size()\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    dt_yearly.plot(marker='o')\n",
    "    plt.title(f\"Yearly Count of {col}\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c13d5bd",
   "metadata": {},
   "source": [
    "## Using a for loop to go through all the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc60ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Employee ID from the columns\n",
    "df = df.drop(columns=['EmployeeID'])\n",
    "\n",
    "# Temporarily turns Attrition and OverTime variables as categories\n",
    "df['Attrition'] = df['Attrition'].astype('category')\n",
    "df['OverTime'] = df['OverTime'].astype('category')\n",
    "\n",
    "# Loop through all columns and run stats, tests, and plots\n",
    "for col in df.columns:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "        dt_stats(df, col)\n",
    "    elif pd.api.types.is_numeric_dtype(df[col]):\n",
    "        qn_stats(df, col)\n",
    "    else:\n",
    "        ql_stats(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b4f3b",
   "metadata": {},
   "source": [
    "## Key Observations:\n",
    "### Gender\n",
    "--- Categorical Summary: Gender ---\n",
    "                   Count  Percentages\n",
    "Female              3091        46.07\n",
    "Male                2968        44.24\n",
    "Non-Binary           589         8.78\n",
    "Prefer Not To Say     61         0.91\n",
    "Unique categories: 4\n",
    "Most frequent: Female\n",
    "- 46.07% of employees identify as female\n",
    "- 44.24% of employees identify as male\n",
    "- 8.78% of employees identify as non-binary\n",
    "- 0.91% of employees did not want to state their identity\n",
    "- There are more employees who identify as female than any other gender.\n",
    "- Not much to say here, I might skip over this one.\n",
    "\n",
    "### Age\n",
    "--- Numerical Summary: Age ---\n",
    "count    6709.000000\n",
    "mean       30.776718\n",
    "std         7.928774\n",
    "min        18.000000\n",
    "25%        25.000000\n",
    "50%        28.000000\n",
    "75%        37.000000\n",
    "max        51.000000\n",
    "- The average age of employees is 31.\n",
    "- The standard deviation of employee's age is 7.93.\n",
    "- The youngest employee is 18 years old.\n",
    "- 25% of employees are under age 25.\n",
    "- 50% of employees are under age 28. \n",
    "- 75% of employees are under age 37.\n",
    "- The oldest employee is 51 years old.\n",
    "\n",
    "Mode: 25\n",
    "Skewness: 0.67\n",
    "Kurtosis: -0.72\n",
    "- The most common age of employees is age 25.\n",
    "- Since the skewness (0.67) is greater than 0, the ages of employees are skewed to the right.\n",
    "- Since the kurtosis (-0.72) is less than 3, the ages of employees are platykurtic. The distribution is flat, and has lighter tails.\n",
    "- The \"Distribution of Age\" plot shows most of the values on the low end, which suggests that employees are more younger. \n",
    "- The histogram plot also shows a peak at 25, showing that most employees are 25 years old in this company. \n",
    "- The plot also shows that after age 30, the spread of employees becomes more even. \n",
    "- The plot also shows that the distribution of the ages of employees is not normal.\n",
    "\n",
    "D'Agostino and Pearson Test:\n",
    " Statistic = 793.6930, p-value = 0.0000\n",
    "Data is not normally distributed.\n",
    "- Since the p-value is very small, we can say that the distribution of age is not normal.\n",
    "- The QQ plot shows that the values deviate from the normal distribution.\n",
    "\n",
    "### BusinessTravel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d7469",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
